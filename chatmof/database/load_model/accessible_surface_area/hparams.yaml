config: !!python/object/new:sacred.config.config_summary.ConfigSummary
  dictitems:
    __doc__: '

      # prepare_data

      max_num_atoms = 300

      min_length = 30

      max_length = 60

      radius = 8

      max_nbr_atoms = 12

      '
    accelerator: auto
    atom_fea_len: 64
    batch_size: 32
    decay_power: 1
    devices: 2
    downstream: accessible_surface_area
    draw_false_grid: false
    drop_rate: 0.1
    end_lr: 0
    exp_name: accessible_surface_area
    hid_dim: 768
    img_size: 30
    in_chans: 1
    learning_rate: 0.0001
    load_path: /home/dudgns1675/anaconda3/envs/chatmof/lib/python3.9/site-packages/moftransformer/database/pmtransformer.ckpt
    log_dir: /storage/dudgns1675/data/moftransformer/finetuning/
    loss_names:
      bbc: 0
      classification: 0
      ggm: 0
      moc: 0
      mpp: 0
      mtp: 0
      regression: 1
      vfp: 0
    lr_mult: 1
    max_epochs: 20
    max_graph_len: 300
    max_grid_len: -1
    max_nbr_atoms: 12
    max_steps: -1
    mean: 1058.4840370909455
    mlp_ratio: 4
    mpp_ratio: 0.15
    n_classes: 0
    nbr_fea_len: 64
    num_heads: 12
    num_layers: 12
    num_nodes: 1
    num_workers: 16
    optim_type: adamw
    patch_size: 5
    per_gpu_batchsize: 16
    precision: 16
    resume_from: null
    root_dataset: /storage/dudgns1675/data/moftransformer/coremof/4_descriptor
    seed: 0
    std: 1142.8825604880274
    test_only: false
    val_check_interval: 1.0
    visualize: false
    warmup_steps: 0.05
    weight_decay: 0.01
  state:
    added: !!set {}
    docs:
      atom_fea_len: 'max_atom_len = 1000  # number of maximum atoms in primitive cell'
      batch_size: desired batch size; for gradient accumulation
      downstream: downstream
      exp_name: model
      hid_dim: transformer setting
      img_size: grid setting
      in_chans: channels of grid image
      lr_mult: multiply lr for downstream heads
      max_graph_len: number of maximum nodes in graph
      max_grid_len: when -1, max_image_len is set to maximum ph*pw of batch images
      max_steps: num_data * max_epoch // batch_size (accumulate_grad_batches)
      mean: normalization target
      num_workers: the number of cpu's core
      optim_type: adamw, adam, sgd (momentum=0.9)
      patch_size: length of patch
      per_gpu_batchsize: you should define this manually with per_gpu_batch_size
      resume_from: PL Trainer Setting
      root_dataset: below params varies with the environment
      seed: the random seed for this experiment
      visualize: return attention map
      warmup_steps: int or float ( max_steps * warmup_steps)
    ignored_fallbacks: !!set {}
    modified: !!set {}
    typechanged: {}
